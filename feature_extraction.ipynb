{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":7426,"status":"ok","timestamp":1742575679801,"user":{"displayName":"Santiago Blas","userId":"12732968484656842116"},"user_tz":180},"id":"eykRlJH0iRiK"},"outputs":[],"source":["import os\n","import pandas as pd\n","import torch\n","import numpy as np\n","import scipy.io as sio\n","from scipy.stats import zscore\n","from scipy.io import loadmat\n","from sklearn.metrics import mutual_info_score , normalized_mutual_info_score\n","from statsmodels.tsa.stattools import grangercausalitytests\n","from concurrent.futures import ProcessPoolExecutor, as_completed\n","from tqdm import tqdm  # Progress bar\n","import logging\n","\n","# ----------------------------\n","# Setup logging for error handling\n","# ----------------------------\n","logging.basicConfig(filename='tensor_processing.log', level=logging.ERROR)\n","\n","# ----------------------------\n","#  Discretization for Mutual Information\n","# ----------------------------\n","def discretize_signals(signals, num_bins=10):\n","    \"\"\"\n","    Discretize continuous signals into a fixed number of bins for MI computation.\n","    \"\"\"\n","    discretized_signals = np.zeros_like(signals)\n","    for i in range(signals.shape[1]):  # Loop over each ROI (column)\n","        discretized_signals[:, i] = np.digitize(\n","            signals[:, i],\n","            bins=np.histogram_bin_edges(signals[:, i], bins=num_bins)\n","        )\n","    return discretized_signals\n","\n","# ----------------------------\n","#  Mutual Information\n","# ----------------------------\n","def compute_mutual_information_matrix(signals, num_bins=10):\n","    \"\"\"\n","    Compute the mutual information matrix for discretized signals.\n","    \"\"\"\n","    discretized_signals = discretize_signals(signals, num_bins)\n","    num_rois = discretized_signals.shape[1]\n","    mi_matrix = np.zeros((num_rois, num_rois))\n","\n","    for i in range(num_rois):\n","        for j in range(num_rois):\n","            mi_matrix[i, j] = normalized_mutual_info_score(\n","                discretized_signals[:, i],\n","                discretized_signals[:, j]\n","            )\n","    return mi_matrix\n","\n","# ----------------------------\n","#  Granger Causality Helpers\n","# ----------------------------\n","def granger_for_pair(args):\n","    \"\"\"\n","    Granger causality function with error handling and logging.\n","    Returns -log(p-value) for the specified lag or 0 on error.\n","    \"\"\"\n","    signals, i, j, max_lag = args\n","    if i != j:\n","        try:\n","            test_result = grangercausalitytests(signals[:, [i, j]], max_lag, verbose=False)\n","            p_value = test_result[max_lag][0]['ssr_ftest'][1]\n","            return i, j, -np.log(p_value) if p_value > 0 else 0\n","        except Exception as e:\n","            logging.error(f\"Error computing Granger causality for pair {i}, {j}: {e}\")\n","            return i, j, 0\n","    return i, j, 0\n","\n","def compute_granger_causality_matrix_parallel(signals, max_lag=1, num_workers=4):\n","    \"\"\"\n","    Compute Granger Causality matrix in parallel using ProcessPoolExecutor.\n","    \"\"\"\n","    num_rois = signals.shape[1]\n","    gc_matrix = np.zeros((num_rois, num_rois))\n","\n","    # Prepare the arguments for parallel computation\n","    args = [(signals, i, j, max_lag) for i in range(num_rois) for j in range(num_rois)]\n","\n","    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n","        futures = [executor.submit(granger_for_pair, arg) for arg in args]\n","        for future in as_completed(futures):\n","            i, j, value = future.result()\n","            gc_matrix[i, j] = value\n","\n","    return gc_matrix\n","\n","# ----------------------------\n","#  Pearson Correlation\n","# ----------------------------\n","def compute_correlation_matrix(signals):\n","    \"\"\"\n","    Compute the Pearson correlation matrix (z-scored by column).\n","    \"\"\"\n","    zscored_signals = zscore(signals, axis=0)\n","    return np.corrcoef(zscored_signals, rowvar=False)  # rowvar=False => correlate columns\n","\n","# ----------------------------\n","#  Dynamic Connectivity (Sliding-Window Variability)\n","# ----------------------------\n","def compute_dynamic_connectivity_variability(signals, win_points=20, step=10):\n","    \"\"\"\n","    1) Splits the signals into overlapping windows.\n","    2) Computes absolute Pearson correlation for each window.\n","    3) Computes Mdiff_mean: mean of absolute differences of consecutive correlation matrices.\n","       Final shape => (Nrois, Nrois).\n","    \"\"\"\n","    # Ensure shape is (Time, ROIs) to match original snippet logic:\n","    #   In the snippet, Mdata is shape (Nrois, Ndata), but the loop does \"Mdata[:, start:start+winPoints]\".\n","    #   So let's just transpose our signals to match that dimension usage.\n","    #   Or we can adapt the code. We'll adapt the code to the shape (Ndata, Nrois).\n","\n","    signals_T = signals  # shape (Ndata, Nrois) already (the code uses signals[:, some_roi])\n","\n","    Ndata, Nrois = signals_T.shape\n","    Nwindows = (Ndata - win_points) // step + 1\n","\n","    # 3D matrix for storing windowed correlation\n","    Mdata_Pearson_3D = np.zeros((Nrois, Nrois, Nwindows))\n","\n","    for idx in range(Nwindows):\n","        start = idx * step\n","        xwin = signals_T[start:start + win_points, :]  # shape (win_points, Nrois)\n","        # Corrcoef wants shape (Nrois, Time) => so we transpose xwin\n","        Mdata_Pearson_3D[:, :, idx] = np.abs(np.corrcoef(xwin.T))\n","\n","    # Differences between successive correlation matrices\n","    Mdiff_3D = np.abs(np.diff(Mdata_Pearson_3D, axis=2))  # shape => (Nrois, Nrois, Nwindows-1)\n","\n","    # Mean across the third dimension\n","    Mdiff_mean = np.mean(Mdiff_3D, axis=2)  # shape => (Nrois, Nrois)\n","\n","    return Mdiff_mean\n","\n","# ----------------------------\n","#  Combine 4 Measures into 3D/4D Tensor\n","# ----------------------------\n","def combine_matrices_to_tensor(corr_matrix, mi_matrix, gc_matrix, dynamic_matrix):\n","    \"\"\"\n","    Stack 4 matrices along a new axis => shape: (4, Nrois, Nrois).\n","    1) corr_matrix\n","    2) mi_matrix\n","    3) gc_matrix\n","    4) dynamic_matrix\n","    \"\"\"\n","    combined_tensor = np.stack([corr_matrix, mi_matrix, gc_matrix, dynamic_matrix], axis=0)\n","    return combined_tensor\n","\n","# ----------------------------\n","#  Memory Usage Helper\n","# ----------------------------\n","def estimate_memory_usage(num_subjects, matrix_shape=(4, 116, 116), dtype=torch.float32):\n","    \"\"\"\n","    Estimate memory usage in GB if you stored all subject tensors in memory.\n","    \"\"\"\n","    bytes_per_element = torch.finfo(dtype).bits // 8\n","    total_elements = np.prod(matrix_shape) * num_subjects\n","    total_bytes = total_elements * bytes_per_element\n","    total_gb = total_bytes / (1024**3)  # Convert to GB\n","    return total_gb\n","\n","# ----------------------------\n","#  Subject-Level Computation\n","# ----------------------------\n","def process_and_save_subject_matrices(subject_id, signals, group, output_dir):\n","    \"\"\"\n","    For each subject:\n","      1) Compute correlation matrix\n","      2) Compute mutual information matrix\n","      3) Compute Granger causality matrix\n","      4) Compute dynamic connectivity variability matrix\n","      5) Combine into a single 4D tensor (4, ROI, ROI)\n","      6) Save to disk\n","    \"\"\"\n","    # 1) Pearson correlation\n","    corr_matrix = compute_correlation_matrix(signals)\n","\n","    # 2) Mutual Information\n","    mi_matrix = compute_mutual_information_matrix(signals)\n","\n","    # 3) Granger Causality\n","    gc_matrix = compute_granger_causality_matrix_parallel(signals)\n","\n","    # 4) Dynamic Connectivity Variability\n","    dynamic_matrix = compute_dynamic_connectivity_variability(signals)\n","\n","    # Combine all into one tensor of shape (4, ROI, ROI)\n","    combined_tensor = combine_matrices_to_tensor(corr_matrix, mi_matrix, gc_matrix, dynamic_matrix)\n","\n","    # Save the tensor to disk\n","    file_name = f'{group}_tensor_{subject_id}.pt'\n","    tensor_path = os.path.join(output_dir, file_name)\n","    torch.save(combined_tensor, tensor_path)\n","\n","    return tensor_path"]},{"cell_type":"markdown","metadata":{"id":"MYEDgs_2kCon"},"source":["\n","#  Main Execution"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1669,"status":"ok","timestamp":1742575681472,"user":{"displayName":"Santiago Blas","userId":"12732968484656842116"},"user_tz":180},"id":"g7CZV09Jkku5","outputId":"fcee9d94-486c-4b92-8c34-b678facec37c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# prompt: import drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1170461,"status":"error","timestamp":1742576851940,"user":{"displayName":"Santiago Blas","userId":"12732968484656842116"},"user_tz":180},"id":"JX2Mp2JCiRiM","outputId":"ef42989f-6c18-47ca-b758-1fd24e91b36d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Constructing tensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352/352 [1:15:35<00:00, 12.89s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","âœ… All tensors successfully computed, saved, and stored.\n","ðŸ”¹ Estimated memory usage for all tensors: 0.07 GB\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import os\n","import pandas as pd\n","import torch\n","import numpy as np\n","import scipy.io as sio\n","from tqdm import tqdm\n","import logging\n","\n","# --- Project configuration ---\n","project_dir = '/home/diego/Escritorio/santiago/1st_paper/116ROIs'\n","os.chdir(project_dir)\n","\n","# Load CSV file\n","csv_path = os.path.join(project_dir, 'DataBaseSubjects.csv')\n","subjects_df = pd.read_csv(csv_path)\n","\n","# Directory for tensor storage\n","output_dir = os.path.join(project_dir, 'TensorData')\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Initialize storage dictionaries\n","tensor_file_paths = {'AD': [], 'CN': [], 'Others': []}\n","\n","# Helper functions (assuming already defined above):\n","# - process_and_save_subject_matrices()\n","# - estimate_memory_usage()\n","\n","subjects = subjects_df['SubjectID'].tolist()\n","groups = subjects_df['ResearchGroup'].tolist()\n","\n","with tqdm(total=len(subjects_df), desc=\"Constructing tensors\") as pbar:\n","    for subject_id, research_group in zip(subjects, groups):\n","        file_path = os.path.join(project_dir, 'ROISignals', f\"ROISignals_{subject_id}.mat\")\n","\n","        if os.path.exists(file_path):\n","            mat_data = sio.loadmat(file_path)\n","            signals = mat_data['ROISignals']\n","\n","            # Ensure signals shape is (Time, ROIs)\n","            if signals.shape[0] < signals.shape[1]:\n","                signals = signals.T\n","\n","            # Limit signals to 116 ROIs if needed\n","            signals = signals[:, :116]\n","\n","            # Group assignment\n","            if research_group == 'AD':\n","                group_label = 'AD'\n","            elif research_group == 'CN':\n","                group_label = 'CN'\n","            else:\n","                group_label = 'Others'\n","\n","            try:\n","                tensor_path = process_and_save_subject_matrices(\n","                    subject_id, signals, group_label, output_dir\n","                )\n","                tensor_file_paths[group_label].append(tensor_path)\n","\n","            except Exception as e:\n","                logging.error(f\"Error processing subject {subject_id}: {e}\")\n","\n","        pbar.update(1)\n","\n","print(\"\\nâœ… All tensors successfully computed, saved, and stored.\")\n","\n","# Memory estimation\n","total_gb = estimate_memory_usage(len(subjects_df))\n","print(f\"ðŸ”¹ Estimated memory usage for all tensors: {total_gb:.2f} GB\")\n","\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
